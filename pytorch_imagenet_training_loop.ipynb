{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a061fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch import nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, dataset, Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets import ImageFolder    \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9804963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6f40b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "28363041",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "        transforms.Resize((288,288)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.AutoAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "        transforms.Resize((288,288)),\n",
    "        transforms.RandomCrop(256),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e117bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ = ImageFolder(\"/home/sahmaran/data/ImageNet/ILSVRC/Data/CLS-LOC/train\", transform=  transforms_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "ab69ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_data_, batch_size = 32, num_workers = 0, shuffle = True);\n",
    "classes_dict = train_data_.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d6f63d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_data(Dataset):\n",
    "    def __init__(self, csv_file = \"/home/sahmaran/data/ImageNet/LOC_val_solution.csv\",\n",
    "                 root_dir = \"/home/sahmaran/data/ImageNet/ILSVRC/Data/CLS-LOC/val\",\n",
    "                 classes_dict = classes_dict, transformations = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.classes_dict = classes_dict\n",
    "        ###\n",
    "        file = pd.read_csv(csv_file)\n",
    "        self.file_names = file.iloc[:,0]\n",
    "        self.anotations = file.iloc[:,-1].apply(self.__split__)\n",
    "        ###\n",
    "        self.transformations = transformations\n",
    "        ###\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.anotations)\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.root_dir, self.file_names[index]+ \".JPEG\")\n",
    "        image = read_image(image_path, )\n",
    "        image = image/255.0\n",
    "        ###\n",
    "        if image.shape[0] == 1:\n",
    "            image = torch.cat((image, image, image), dim = 0)\n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        ###\n",
    "        return image, self.classes_dict[self.anotations[index]]\n",
    "        \n",
    "    def __split__(self, n):\n",
    "        return n.split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "5695e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_ = DataLoader(test_data(transformations = transforms_test), batch_size = 32, num_workers = 0, );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "71c7644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "12c77426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): MobileViTForImageClassification(\n",
       "    (mobilevit): MobileViTModel(\n",
       "      (conv_stem): MobileViTConvLayer(\n",
       "        (convolution): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (normalization): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): SiLUActivation()\n",
       "      )\n",
       "      (encoder): MobileViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): MobileViTMobileNetLayer(\n",
       "            (layer): ModuleList(\n",
       "              (0): MobileViTInvertedResidual(\n",
       "                (expand_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (conv_3x3): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "                  (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (reduce_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): MobileViTMobileNetLayer(\n",
       "            (layer): ModuleList(\n",
       "              (0): MobileViTInvertedResidual(\n",
       "                (expand_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (conv_3x3): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "                  (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (reduce_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (1): MobileViTInvertedResidual(\n",
       "                (expand_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (conv_3x3): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "                  (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (reduce_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "              (2): MobileViTInvertedResidual(\n",
       "                (expand_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (conv_3x3): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "                  (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): SiLUActivation()\n",
       "                )\n",
       "                (reduce_1x1): MobileViTConvLayer(\n",
       "                  (convolution): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): MobileViTLayer(\n",
       "            (downsampling_layer): MobileViTInvertedResidual(\n",
       "              (expand_1x1): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): SiLUActivation()\n",
       "              )\n",
       "              (conv_3x3): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "                (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): SiLUActivation()\n",
       "              )\n",
       "              (reduce_1x1): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (conv_kxk): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "            (conv_1x1): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (transformer): MobileViTTransformer(\n",
       "              (layer): ModuleList(\n",
       "                (0): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=64, out_features=128, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (1): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=64, out_features=128, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=128, out_features=64, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (conv_projection): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "            (fusion): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (3): MobileViTLayer(\n",
       "            (downsampling_layer): MobileViTInvertedResidual(\n",
       "              (expand_1x1): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): SiLUActivation()\n",
       "              )\n",
       "              (conv_3x3): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "                (normalization): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): SiLUActivation()\n",
       "              )\n",
       "              (reduce_1x1): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (conv_kxk): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "            (conv_1x1): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (transformer): MobileViTTransformer(\n",
       "              (layer): ModuleList(\n",
       "                (0): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (key): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (value): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=80, out_features=160, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=160, out_features=80, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (1): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (key): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (value): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=80, out_features=160, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=160, out_features=80, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (2): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (key): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (value): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=80, out_features=160, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=160, out_features=80, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (3): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (key): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (value): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=80, out_features=80, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=80, out_features=160, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=160, out_features=80, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layernorm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "            (conv_projection): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "            (fusion): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "          (4): MobileViTLayer(\n",
       "            (downsampling_layer): MobileViTInvertedResidual(\n",
       "              (expand_1x1): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): SiLUActivation()\n",
       "              )\n",
       "              (conv_3x3): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "                (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): SiLUActivation()\n",
       "              )\n",
       "              (reduce_1x1): MobileViTConvLayer(\n",
       "                (convolution): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (conv_kxk): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "            (conv_1x1): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (transformer): MobileViTTransformer(\n",
       "              (layer): ModuleList(\n",
       "                (0): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=192, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=192, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (1): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=192, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=192, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (2): MobileViTTransformerLayer(\n",
       "                  (attention): MobileViTAttention(\n",
       "                    (attention): MobileViTSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): MobileViTSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (intermediate): MobileViTIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=192, bias=True)\n",
       "                    (intermediate_act_fn): SiLUActivation()\n",
       "                  )\n",
       "                  (output): MobileViTOutput(\n",
       "                    (dense): Linear(in_features=192, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layernorm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (conv_projection): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "            (fusion): MobileViTConvLayer(\n",
       "              (convolution): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_1x1_exp): MobileViTConvLayer(\n",
       "        (convolution): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (normalization): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): SiLUActivation()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=True)\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.2, inplace=False)\n",
       "      (1): Linear(in_features=320, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier = nn.Sequential(nn.Dropout(0.2), \n",
    "                                 nn.Linear(320, 1000, bias = True))\n",
    "if torch.cuda.device_count() > 1:\n",
    "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "9e473ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "71d65e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.9134633541107178: 100%|███████████████| 40037/40037 [3:12:35<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch passed, validation acc is 0.3525271912987844, train loss is 4.52855388319099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.4410736560821533: 100%|███████████████| 40037/40037 [3:10:48<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th epoch passed, validation acc is 0.4599728087012156, train loss is 3.0142564069296682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.2609126567840576: 100%|███████████████| 40037/40037 [3:10:42<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2th epoch passed, validation acc is 0.5080974088291746, train loss is 2.6269749096081436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.0236284732818604: 100%|███████████████| 40037/40037 [3:10:42<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3th epoch passed, validation acc is 0.5235724568138196, train loss is 2.4217881180414307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.8066599369049072: 100%|███████████████| 40037/40037 [3:10:34<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th epoch passed, validation acc is 0.552643154190659, train loss is 2.286628150563588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.6710119247436523: 100%|███████████████| 40037/40037 [3:12:29<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th epoch passed, validation acc is 0.5634396992962252, train loss is 2.1867813916685375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.6322369575500488: 100%|███████████████| 40037/40037 [3:10:29<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6th epoch passed, validation acc is 0.5750359884836852, train loss is 2.112310324328987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.8335164785385132:  87%|█████████████  | 34702/40037 [2:45:04<25:22,  3.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12122/4150813952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \"\"\"\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2080\u001b[0m                 )\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Train Loop starts here\"\"\"\n",
    "for i in range(25):\n",
    "    K = []\n",
    "    loss = []\n",
    "    Loss = []\n",
    "    bar = tqdm(train_data)\n",
    "    for x,y in bar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        y_pred = model(x)[\"logits\"]\n",
    "        \n",
    "        loss = nn.CrossEntropyLoss()(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        Loss.append(loss.item())\n",
    "        bar.set_description(f\"{loss.item()}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y in test_data_:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            K.append(sum(torch.argmax(model(x)[\"logits\"],-1) == y).item()/len(y))\n",
    "    print(f\"{i}th epoch passed, validation acc is {np.mean(K)}, train loss is {np.mean(Loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c8b4b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/sahmaran/Dropbox/Machines_learning/vit/model_train/model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2c7a56f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sahmaran/Dropbox/Machines_learning/vit/model_train'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ff892346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2084/2084 [06:07<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch passed, validation acc is 0.0009596928982725527, train loss is 6.8178387895415105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for x,y in tqdm(test_data_):\n",
    "        x = x.cuda(1)\n",
    "        y = y.cuda(1)\n",
    "        K.append(sum(torch.argmax(model(x)[\"logits\"],-1) == y).item()/len(y))\n",
    "print(f\"{i}th epoch passed, validation acc is {np.mean(K)}, train loss is {np.mean(Loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
