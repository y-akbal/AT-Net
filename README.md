# AT-NET

Here is a variation on Mobile-Net architecture, I have added some intermediate attention layers that are a bit different 
than the conventional attention layers, motivated by Patches are all you need paper.

I have reached --- accuracy on ---, and guess this happened where, at home on two GPUs!!!

